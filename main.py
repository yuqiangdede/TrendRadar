# coding=utf-8

import json
import os
import random
import re
import time
import webbrowser
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union

import pytz
import requests
import yaml


VERSION = "3.0.5"


# === é…ç½®ç®¡ç† ===
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.environ.get("CONFIG_PATH", "config/config.yaml")

    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨")

    with open(config_path, "r", encoding="utf-8") as f:
        config_data = yaml.safe_load(f)

    print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")

    # æ„å»ºé…ç½®
    config = {
        "REQUEST_INTERVAL": config_data["crawler"]["request_interval"],
        "REPORT_MODE": os.environ.get("REPORT_MODE", "").strip()
        or config_data["report"]["mode"],
        "RANK_THRESHOLD": config_data["report"]["rank_threshold"],
        "USE_PROXY": config_data["crawler"]["use_proxy"],
        "DEFAULT_PROXY": config_data["crawler"]["default_proxy"],
        "ENABLE_CRAWLER": os.environ.get("ENABLE_CRAWLER", "").strip().lower()
        in ("true", "1")
        if os.environ.get("ENABLE_CRAWLER", "").strip()
        else config_data["crawler"]["enable_crawler"],
        "ENABLE_NOTIFICATION": os.environ.get("ENABLE_NOTIFICATION", "").strip().lower()
        in ("true", "1")
        if os.environ.get("ENABLE_NOTIFICATION", "").strip()
        else config_data["notification"]["enable_notification"],
        "MESSAGE_BATCH_SIZE": config_data["notification"]["message_batch_size"],
        "BATCH_SEND_INTERVAL": config_data["notification"]["batch_send_interval"],
        "MESSAGE_SEPARATOR": config_data["notification"]["message_separator"],
        "PUSH_WINDOW": {
            "ENABLED": os.environ.get("PUSH_WINDOW_ENABLED", "").strip().lower()
            in ("true", "1")
            if os.environ.get("PUSH_WINDOW_ENABLED", "").strip()
            else config_data["notification"]
            .get("push_window", {})
            .get("enabled", False),
            "TIME_RANGE": {
                "START": os.environ.get("PUSH_WINDOW_START", "").strip()
                or config_data["notification"]
                .get("push_window", {})
                .get("time_range", {})
                .get("start", "08:00"),
                "END": os.environ.get("PUSH_WINDOW_END", "").strip()
                or config_data["notification"]
                .get("push_window", {})
                .get("time_range", {})
                .get("end", "22:00"),
            },
            "ONCE_PER_DAY": os.environ.get("PUSH_WINDOW_ONCE_PER_DAY", "").strip().lower()
            in ("true", "1")
            if os.environ.get("PUSH_WINDOW_ONCE_PER_DAY", "").strip()
            else config_data["notification"]
            .get("push_window", {})
            .get("once_per_day", True),
            "RECORD_RETENTION_DAYS": int(
                os.environ.get("PUSH_WINDOW_RETENTION_DAYS", "").strip() or "0"
            )
            or config_data["notification"]
            .get("push_window", {})
            .get("push_record_retention_days", 7),
        },
        "WEIGHT_CONFIG": {
            "RANK_WEIGHT": config_data["weight"]["rank_weight"],
            "FREQUENCY_WEIGHT": config_data["weight"]["frequency_weight"],
            "HOTNESS_WEIGHT": config_data["weight"]["hotness_weight"],
        },
        "PLATFORMS": config_data["platforms"],
    }

    # é€šçŸ¥æ¸ é“é…ç½®ï¼ˆç¯å¢ƒå˜é‡ä¼˜å…ˆï¼‰
    # é€šçŸ¥ç›¸å…³é…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
    notification = config_data.get("notification", {})
    webhooks = notification.get("webhooks", {})

    config["WEWORK_WEBHOOK_URL"] = os.environ.get(
        "WEWORK_WEBHOOK_URL", ""
    ).strip() or webhooks.get("wework_url", "")

    notification_sources = []
    if config["WEWORK_WEBHOOK_URL"]:
        source = "ç¯å¢ƒå˜é‡" if os.environ.get("WEWORK_WEBHOOK_URL") else "é…ç½®æ–‡ä»¶"
        notification_sources.append(f"ä¼ä¸šå¾®ä¿¡({source})")

    if notification_sources:
        print(f"é€šçŸ¥å¯ç”¨æ¥æº: {', '.join(notification_sources)}")
    else:
        print("æœªé…ç½®ä»»ä½•é€šçŸ¥æ¸ é“")

    return config
    return config


print("æ­£åœ¨åŠ è½½é…ç½®...")
CONFIG = load_config()
print(f"TrendRadar v{VERSION} é…ç½®åŠ è½½å®Œæˆ")
print(f"ç›‘æ§å¹³å°æ•°é‡: {len(CONFIG['PLATFORMS'])}")


# === å·¥å…·å‡½æ•° ===
def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    return datetime.now(pytz.timezone("Asia/Shanghai"))


def format_date_folder():
    """æ ¼å¼åŒ–æ—¥æœŸæ–‡ä»¶å¤¹"""
    return get_beijing_time().strftime("%Yå¹´%mæœˆ%dæ—¥")


def format_time_filename():
    """æ ¼å¼åŒ–æ—¶é—´æ–‡ä»¶å"""
    return get_beijing_time().strftime("%Hæ—¶%Måˆ†")


def clean_title(title: str) -> str:
    """æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
    if not isinstance(title, str):
        title = str(title)
    cleaned_title = title.replace("\n", " ").replace("\r", " ")
    cleaned_title = re.sub(r"\s+", " ", cleaned_title)
    cleaned_title = cleaned_title.strip()
    return cleaned_title


def ensure_directory_exists(directory: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(directory).mkdir(parents=True, exist_ok=True)


def get_output_path(subfolder: str, filename: str) -> str:
    """è·å–è¾“å‡ºè·¯å¾„"""
    date_folder = format_date_folder()
    output_dir = Path("output") / date_folder / subfolder
    ensure_directory_exists(str(output_dir))
    return str(output_dir / filename)



def is_first_crawl_today() -> bool:
    """æ£€æµ‹æ˜¯å¦æ˜¯å½“å¤©ç¬¬ä¸€æ¬¡çˆ¬å–"""
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"

    if not txt_dir.exists():
        return True

    files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])
    return len(files) <= 1


def html_escape(text: str) -> str:
    """HTMLè½¬ä¹‰"""
    if not isinstance(text, str):
        text = str(text)

    return (
        text.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
        .replace("'", "&#x27;")
    )


# === æ¨é€è®°å½•ç®¡ç† ===
class PushRecordManager:
    """æ¨é€è®°å½•ç®¡ç†å™¨"""

    def __init__(self):
        self.record_dir = Path("output") / ".push_records"
        self.ensure_record_dir()
        self.cleanup_old_records()

    def ensure_record_dir(self):
        """ç¡®ä¿è®°å½•ç›®å½•å­˜åœ¨"""
        self.record_dir.mkdir(parents=True, exist_ok=True)

    def get_today_record_file(self) -> Path:
        """è·å–ä»Šå¤©çš„è®°å½•æ–‡ä»¶è·¯å¾„"""
        today = get_beijing_time().strftime("%Y%m%d")
        return self.record_dir / f"push_record_{today}.json"

    def cleanup_old_records(self):
        """æ¸…ç†è¿‡æœŸçš„æ¨é€è®°å½•"""
        retention_days = CONFIG["PUSH_WINDOW"]["RECORD_RETENTION_DAYS"]
        current_time = get_beijing_time()

        for record_file in self.record_dir.glob("push_record_*.json"):
            try:
                date_str = record_file.stem.replace("push_record_", "")
                file_date = datetime.strptime(date_str, "%Y%m%d")
                file_date = pytz.timezone("Asia/Shanghai").localize(file_date)

                if (current_time - file_date).days > retention_days:
                    record_file.unlink()
                    print(f"æ¸…ç†è¿‡æœŸæ¨é€è®°å½•: {record_file.name}")
            except Exception as e:
                print(f"æ¸…ç†è®°å½•æ–‡ä»¶å¤±è´¥ {record_file}: {e}")

    def has_pushed_today(self) -> bool:
        """æ£€æŸ¥ä»Šå¤©æ˜¯å¦å·²ç»æ¨é€è¿‡"""
        record_file = self.get_today_record_file()

        if not record_file.exists():
            return False

        try:
            with open(record_file, "r", encoding="utf-8") as f:
                record = json.load(f)
            return record.get("pushed", False)
        except Exception as e:
            print(f"è¯»å–æ¨é€è®°å½•å¤±è´¥: {e}")
            return False

    def record_push(self, report_type: str):
        """è®°å½•æ¨é€"""
        record_file = self.get_today_record_file()
        now = get_beijing_time()

        record = {
            "pushed": True,
            "push_time": now.strftime("%Y-%m-%d %H:%M:%S"),
            "report_type": report_type,
        }

        try:
            with open(record_file, "w", encoding="utf-8") as f:
                json.dump(record, f, ensure_ascii=False, indent=2)
            print(f"æ¨é€è®°å½•å·²ä¿å­˜: {report_type} at {now.strftime('%H:%M:%S')}")
        except Exception as e:
            print(f"ä¿å­˜æ¨é€è®°å½•å¤±è´¥: {e}")

    def is_in_time_range(self, start_time: str, end_time: str) -> bool:
        """æ£€æŸ¥å½“å‰æ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…"""
        now = get_beijing_time()
        current_time = now.strftime("%H:%M")
    
        def normalize_time(time_str: str) -> str:
            """å°†æ—¶é—´å­—ç¬¦ä¸²æ ‡å‡†åŒ–ä¸º HH:MM æ ¼å¼"""
            try:
                parts = time_str.strip().split(":")
                if len(parts) != 2:
                    raise ValueError(f"æ—¶é—´æ ¼å¼é”™è¯¯: {time_str}")
            
                hour = int(parts[0])
                minute = int(parts[1])
            
                if not (0 <= hour <= 23 and 0 <= minute <= 59):
                    raise ValueError(f"æ—¶é—´èŒƒå›´é”™è¯¯: {time_str}")
            
                return f"{hour:02d}:{minute:02d}"
            except Exception as e:
                print(f"æ—¶é—´æ ¼å¼åŒ–é”™è¯¯ '{time_str}': {e}")
                return time_str
    
        normalized_start = normalize_time(start_time)
        normalized_end = normalize_time(end_time)
        normalized_current = normalize_time(current_time)
    
        result = normalized_start <= normalized_current <= normalized_end
    
        if not result:
            print(f"æ—¶é—´çª—å£åˆ¤æ–­ï¼šå½“å‰ {normalized_current}ï¼Œçª—å£ {normalized_start}-{normalized_end}")
    
        return result


# === æ•°æ®è·å– ===
class DataFetcher:
    """æ•°æ®è·å–å™¨"""

    def __init__(self, proxy_url: Optional[str] = None):
        self.proxy_url = proxy_url

    def fetch_data(
        self,
        id_info: Union[str, Tuple[str, str]],
        max_retries: int = 2,
        min_retry_wait: int = 3,
        max_retry_wait: int = 5,
    ) -> Tuple[Optional[str], str, str]:
        """è·å–æŒ‡å®šIDæ•°æ®ï¼Œæ”¯æŒé‡è¯•"""
        if isinstance(id_info, tuple):
            id_value, alias = id_info
        else:
            id_value = id_info
            alias = id_value

        url = f"https://newsnow.busiyi.world/api/s?id={id_value}&latest"

        proxies = None
        if self.proxy_url:
            proxies = {"http": self.proxy_url, "https": self.proxy_url}

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Cache-Control": "no-cache",
        }

        retries = 0
        while retries <= max_retries:
            try:
                response = requests.get(
                    url, proxies=proxies, headers=headers, timeout=10
                )
                response.raise_for_status()

                data_text = response.text
                data_json = json.loads(data_text)

                status = data_json.get("status", "æœªçŸ¥")
                if status not in ["success", "cache"]:
                    raise ValueError(f"å“åº”çŠ¶æ€å¼‚å¸¸: {status}")

                status_info = "æœ€æ–°æ•°æ®" if status == "success" else "ç¼“å­˜æ•°æ®"
                print(f"è·å– {id_value} æˆåŠŸï¼ˆ{status_info}ï¼‰")
                return data_text, id_value, alias

            except Exception as e:
                retries += 1
                if retries <= max_retries:
                    base_wait = random.uniform(min_retry_wait, max_retry_wait)
                    additional_wait = (retries - 1) * random.uniform(1, 2)
                    wait_time = base_wait + additional_wait
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}. {wait_time:.2f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}")
                    return None, id_value, alias
        return None, id_value, alias

    def crawl_websites(
        self,
        ids_list: List[Union[str, Tuple[str, str]]],
        request_interval: int = CONFIG["REQUEST_INTERVAL"],
    ) -> Tuple[Dict, Dict, List]:
        """çˆ¬å–å¤šä¸ªç½‘ç«™æ•°æ®"""
        results = {}
        id_to_name = {}
        failed_ids = []

        for i, id_info in enumerate(ids_list):
            if isinstance(id_info, tuple):
                id_value, name = id_info
            else:
                id_value = id_info
                name = id_value

            id_to_name[id_value] = name
            response, _, _ = self.fetch_data(id_info)

            if response:
                try:
                    data = json.loads(response)
                    results[id_value] = {}
                    for index, item in enumerate(data.get("items", []), 1):
                        title = item["title"]
                        url = item.get("url", "")
                        mobile_url = item.get("mobileUrl", "")

                        if title in results[id_value]:
                            results[id_value][title]["ranks"].append(index)
                        else:
                            results[id_value][title] = {
                                "ranks": [index],
                                "url": url,
                                "mobileUrl": mobile_url,
                            }
                except json.JSONDecodeError:
                    print(f"è§£æ {id_value} å“åº”å¤±è´¥")
                    failed_ids.append(id_value)
                except Exception as e:
                    print(f"å¤„ç† {id_value} æ•°æ®å‡ºé”™: {e}")
                    failed_ids.append(id_value)
            else:
                failed_ids.append(id_value)

            if i < len(ids_list) - 1:
                actual_interval = request_interval + random.randint(-10, 20)
                actual_interval = max(50, actual_interval)
                time.sleep(actual_interval / 1000)

        print(f"æˆåŠŸ: {list(results.keys())}, å¤±è´¥: {failed_ids}")
        return results, id_to_name, failed_ids


# === æ•°æ®å¤„ç† ===
def save_titles_to_file(results: Dict, id_to_name: Dict, failed_ids: List) -> str:
    """ä¿å­˜æ ‡é¢˜åˆ°æ–‡ä»¶"""
    file_path = get_output_path("txt", f"{format_time_filename()}.txt")

    with open(file_path, "w", encoding="utf-8") as f:
        for id_value, title_data in results.items():
            # id | name æˆ– id
            name = id_to_name.get(id_value)
            if name and name != id_value:
                f.write(f"{id_value} | {name}\n")
            else:
                f.write(f"{id_value}\n")

            # æŒ‰æ’åæ’åºæ ‡é¢˜
            sorted_titles = []
            for title, info in title_data.items():
                cleaned_title = clean_title(title)
                if isinstance(info, dict):
                    ranks = info.get("ranks", [])
                    url = info.get("url", "")
                    mobile_url = info.get("mobileUrl", "")
                else:
                    ranks = info if isinstance(info, list) else []
                    url = ""
                    mobile_url = ""

                rank = ranks[0] if ranks else 1
                sorted_titles.append((rank, cleaned_title, url, mobile_url))

            sorted_titles.sort(key=lambda x: x[0])

            for rank, cleaned_title, url, mobile_url in sorted_titles:
                line = f"{rank}. {cleaned_title}"

                if url:
                    line += f" [URL:{url}]"
                if mobile_url:
                    line += f" [MOBILE:{mobile_url}]"
                f.write(line + "\n")

            f.write("\n")

        if failed_ids:
            f.write("==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====\n")
            for id_value in failed_ids:
                f.write(f"{id_value}\n")

    return file_path


def load_frequency_words(
    frequency_file: Optional[str] = None,
) -> Tuple[List[Dict], List[str]]:
    """åŠ è½½é¢‘ç‡è¯é…ç½®"""
    if frequency_file is None:
        frequency_file = os.environ.get(
            "FREQUENCY_WORDS_PATH", "config/frequency_words.txt"
        )

    frequency_path = Path(frequency_file)
    if not frequency_path.exists():
        raise FileNotFoundError(f"é¢‘ç‡è¯æ–‡ä»¶ {frequency_file} ä¸å­˜åœ¨")

    with open(frequency_path, "r", encoding="utf-8") as f:
        content = f.read()

    word_groups = [group.strip() for group in content.split("\n\n") if group.strip()]

    processed_groups = []
    filter_words = []

    for group in word_groups:
        words = [word.strip() for word in group.split("\n") if word.strip()]

        group_required_words = []
        group_normal_words = []
        group_filter_words = []

        for word in words:
            if word.startswith("!"):
                filter_words.append(word[1:])
                group_filter_words.append(word[1:])
            elif word.startswith("+"):
                group_required_words.append(word[1:])
            else:
                group_normal_words.append(word)

        if group_required_words or group_normal_words:
            if group_normal_words:
                group_key = " ".join(group_normal_words)
            else:
                group_key = " ".join(group_required_words)

            processed_groups.append(
                {
                    "required": group_required_words,
                    "normal": group_normal_words,
                    "group_key": group_key,
                }
            )

    return processed_groups, filter_words


def parse_file_titles(file_path: Path) -> Tuple[Dict, Dict]:
    """è§£æå•ä¸ªtxtæ–‡ä»¶çš„æ ‡é¢˜æ•°æ®ï¼Œè¿”å›(titles_by_id, id_to_name)"""
    titles_by_id = {}
    id_to_name = {}

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
        sections = content.split("\n\n")

        for section in sections:
            if not section.strip() or "==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====" in section:
                continue

            lines = section.strip().split("\n")
            if len(lines) < 2:
                continue

            # id | name æˆ– id
            header_line = lines[0].strip()
            if " | " in header_line:
                parts = header_line.split(" | ", 1)
                source_id = parts[0].strip()
                name = parts[1].strip()
                id_to_name[source_id] = name
            else:
                source_id = header_line
                id_to_name[source_id] = source_id

            titles_by_id[source_id] = {}

            for line in lines[1:]:
                if line.strip():
                    try:
                        title_part = line.strip()
                        rank = None

                        # æå–æ’å
                        if ". " in title_part and title_part.split(". ")[0].isdigit():
                            rank_str, title_part = title_part.split(". ", 1)
                            rank = int(rank_str)

                        # æå– MOBILE URL
                        mobile_url = ""
                        if " [MOBILE:" in title_part:
                            title_part, mobile_part = title_part.rsplit(" [MOBILE:", 1)
                            if mobile_part.endswith("]"):
                                mobile_url = mobile_part[:-1]

                        # æå– URL
                        url = ""
                        if " [URL:" in title_part:
                            title_part, url_part = title_part.rsplit(" [URL:", 1)
                            if url_part.endswith("]"):
                                url = url_part[:-1]

                        title = clean_title(title_part.strip())
                        ranks = [rank] if rank is not None else [1]

                        titles_by_id[source_id][title] = {
                            "ranks": ranks,
                            "url": url,
                            "mobileUrl": mobile_url,
                        }

                    except Exception as e:
                        print(f"è§£ææ ‡é¢˜è¡Œå‡ºé”™: {line}, é”™è¯¯: {e}")

    return titles_by_id, id_to_name


def read_all_today_titles(
    current_platform_ids: Optional[List[str]] = None,
) -> Tuple[Dict, Dict, Dict]:
    """è¯»å–å½“å¤©æ‰€æœ‰æ ‡é¢˜æ–‡ä»¶ï¼Œæ”¯æŒæŒ‰å½“å‰ç›‘æ§å¹³å°è¿‡æ»¤"""
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"

    if not txt_dir.exists():
        return {}, {}, {}

    all_results = {}
    final_id_to_name = {}
    title_info = {}

    files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])

    for file_path in files:
        time_info = file_path.stem

        titles_by_id, file_id_to_name = parse_file_titles(file_path)

        if current_platform_ids is not None:
            filtered_titles_by_id = {}
            filtered_id_to_name = {}

            for source_id, title_data in titles_by_id.items():
                if source_id in current_platform_ids:
                    filtered_titles_by_id[source_id] = title_data
                    if source_id in file_id_to_name:
                        filtered_id_to_name[source_id] = file_id_to_name[source_id]

            titles_by_id = filtered_titles_by_id
            file_id_to_name = filtered_id_to_name

        final_id_to_name.update(file_id_to_name)

        for source_id, title_data in titles_by_id.items():
            process_source_data(
                source_id, title_data, time_info, all_results, title_info
            )

    return all_results, final_id_to_name, title_info


def process_source_data(
    source_id: str,
    title_data: Dict,
    time_info: str,
    all_results: Dict,
    title_info: Dict,
) -> None:
    """å¤„ç†æ¥æºæ•°æ®ï¼Œåˆå¹¶é‡å¤æ ‡é¢˜"""
    if source_id not in all_results:
        all_results[source_id] = title_data

        if source_id not in title_info:
            title_info[source_id] = {}

        for title, data in title_data.items():
            ranks = data.get("ranks", [])
            url = data.get("url", "")
            mobile_url = data.get("mobileUrl", "")

            title_info[source_id][title] = {
                "first_time": time_info,
                "last_time": time_info,
                "count": 1,
                "ranks": ranks,
                "url": url,
                "mobileUrl": mobile_url,
            }
    else:
        for title, data in title_data.items():
            ranks = data.get("ranks", [])
            url = data.get("url", "")
            mobile_url = data.get("mobileUrl", "")

            if title not in all_results[source_id]:
                all_results[source_id][title] = {
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }
                title_info[source_id][title] = {
                    "first_time": time_info,
                    "last_time": time_info,
                    "count": 1,
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }
            else:
                existing_data = all_results[source_id][title]
                existing_ranks = existing_data.get("ranks", [])
                existing_url = existing_data.get("url", "")
                existing_mobile_url = existing_data.get("mobileUrl", "")

                merged_ranks = existing_ranks.copy()
                for rank in ranks:
                    if rank not in merged_ranks:
                        merged_ranks.append(rank)

                all_results[source_id][title] = {
                    "ranks": merged_ranks,
                    "url": existing_url or url,
                    "mobileUrl": existing_mobile_url or mobile_url,
                }

                title_info[source_id][title]["last_time"] = time_info
                title_info[source_id][title]["ranks"] = merged_ranks
                title_info[source_id][title]["count"] += 1
                if not title_info[source_id][title].get("url"):
                    title_info[source_id][title]["url"] = url
                if not title_info[source_id][title].get("mobileUrl"):
                    title_info[source_id][title]["mobileUrl"] = mobile_url


def detect_latest_new_titles(current_platform_ids: Optional[List[str]] = None) -> Dict:
    """æ£€æµ‹å½“æ—¥æœ€æ–°æ‰¹æ¬¡çš„æ–°å¢æ ‡é¢˜ï¼Œæ”¯æŒæŒ‰å½“å‰ç›‘æ§å¹³å°è¿‡æ»¤"""
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"

    if not txt_dir.exists():
        return {}

    files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])
    if len(files) < 2:
        return {}

    # è§£ææœ€æ–°æ–‡ä»¶
    latest_file = files[-1]
    latest_titles, _ = parse_file_titles(latest_file)

    # å¦‚æœæŒ‡å®šäº†å½“å‰å¹³å°åˆ—è¡¨ï¼Œè¿‡æ»¤æœ€æ–°æ–‡ä»¶æ•°æ®
    if current_platform_ids is not None:
        filtered_latest_titles = {}
        for source_id, title_data in latest_titles.items():
            if source_id in current_platform_ids:
                filtered_latest_titles[source_id] = title_data
        latest_titles = filtered_latest_titles

    # æ±‡æ€»å†å²æ ‡é¢˜ï¼ˆæŒ‰å¹³å°è¿‡æ»¤ï¼‰
    historical_titles = {}
    for file_path in files[:-1]:
        historical_data, _ = parse_file_titles(file_path)

        # è¿‡æ»¤å†å²æ•°æ®
        if current_platform_ids is not None:
            filtered_historical_data = {}
            for source_id, title_data in historical_data.items():
                if source_id in current_platform_ids:
                    filtered_historical_data[source_id] = title_data
            historical_data = filtered_historical_data

        for source_id, titles_data in historical_data.items():
            if source_id not in historical_titles:
                historical_titles[source_id] = set()
            for title in titles_data.keys():
                historical_titles[source_id].add(title)

    # æ‰¾å‡ºæ–°å¢æ ‡é¢˜
    new_titles = {}
    for source_id, latest_source_titles in latest_titles.items():
        historical_set = historical_titles.get(source_id, set())
        source_new_titles = {}

        for title, title_data in latest_source_titles.items():
            if title not in historical_set:
                source_new_titles[title] = title_data

        if source_new_titles:
            new_titles[source_id] = source_new_titles

    return new_titles


# === ç»Ÿè®¡å’Œåˆ†æ ===
def calculate_news_weight(
    title_data: Dict, rank_threshold: int = CONFIG["RANK_THRESHOLD"]
) -> float:
    """è®¡ç®—æ–°é—»æƒé‡ï¼Œç”¨äºæ’åº"""
    ranks = title_data.get("ranks", [])
    if not ranks:
        return 0.0

    count = title_data.get("count", len(ranks))
    weight_config = CONFIG["WEIGHT_CONFIG"]

    # æ’åæƒé‡ï¼šÎ£(11 - min(rank, 10)) / å‡ºç°æ¬¡æ•°
    rank_scores = []
    for rank in ranks:
        score = 11 - min(rank, 10)
        rank_scores.append(score)

    rank_weight = sum(rank_scores) / len(ranks) if ranks else 0

    # é¢‘æ¬¡æƒé‡ï¼šmin(å‡ºç°æ¬¡æ•°, 10) Ã— 10
    frequency_weight = min(count, 10) * 10

    # çƒ­åº¦åŠ æˆï¼šé«˜æ’åæ¬¡æ•° / æ€»å‡ºç°æ¬¡æ•° Ã— 100
    high_rank_count = sum(1 for rank in ranks if rank <= rank_threshold)
    hotness_ratio = high_rank_count / len(ranks) if ranks else 0
    hotness_weight = hotness_ratio * 100

    total_weight = (
        rank_weight * weight_config["RANK_WEIGHT"]
        + frequency_weight * weight_config["FREQUENCY_WEIGHT"]
        + hotness_weight * weight_config["HOTNESS_WEIGHT"]
    )

    return total_weight


def matches_word_groups(
    title: str, word_groups: List[Dict], filter_words: List[str]
) -> bool:
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ¹é…è¯ç»„è§„åˆ™"""
    # å¦‚æœæ²¡æœ‰é…ç½®è¯ç»„ï¼Œåˆ™åŒ¹é…æ‰€æœ‰æ ‡é¢˜ï¼ˆæ”¯æŒæ˜¾ç¤ºå…¨éƒ¨æ–°é—»ï¼‰
    if not word_groups:
        return True

    title_lower = title.lower()

    # è¿‡æ»¤è¯æ£€æŸ¥
    if any(filter_word.lower() in title_lower for filter_word in filter_words):
        return False

    # è¯ç»„åŒ¹é…æ£€æŸ¥
    for group in word_groups:
        required_words = group["required"]
        normal_words = group["normal"]

        # å¿…é¡»è¯æ£€æŸ¥
        if required_words:
            all_required_present = all(
                req_word.lower() in title_lower for req_word in required_words
            )
            if not all_required_present:
                continue

        # æ™®é€šè¯æ£€æŸ¥
        if normal_words:
            any_normal_present = any(
                normal_word.lower() in title_lower for normal_word in normal_words
            )
            if not any_normal_present:
                continue

        return True

    return False


def format_time_display(first_time: str, last_time: str) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if not first_time:
        return ""
    if first_time == last_time or not last_time:
        return first_time
    else:
        return f"[{first_time} ~ {last_time}]"


def format_rank_display(ranks: List[int], rank_threshold: int, format_type: str) -> str:
    """Unified rank display for WeCom and HTML"""
    if not ranks:
        return ""

    unique_ranks = sorted(set(ranks))
    min_rank = unique_ranks[0]
    max_rank = unique_ranks[-1]

    if format_type == "html":
        highlight_start = "<font color='red'><strong>"
        highlight_end = "</strong></font>"
    else:
        highlight_start = "**"
        highlight_end = "**"

    if min_rank <= rank_threshold:
        if min_rank == max_rank:
            return f"{highlight_start}[{min_rank}]{highlight_end}"
        else:
            return f"{highlight_start}[{min_rank} - {max_rank}]{highlight_end}"
    else:
        if min_rank == max_rank:
            return f"[{min_rank}]"
        else:
            return f"[{min_rank} - {max_rank}]"



def count_word_frequency(
    results: Dict,
    word_groups: List[Dict],
    filter_words: List[str],
    id_to_name: Dict,
    title_info: Optional[Dict] = None,
    rank_threshold: int = CONFIG["RANK_THRESHOLD"],
    new_titles: Optional[Dict] = None,
    mode: str = "daily",
) -> Tuple[List[Dict], int]:
    """ç»Ÿè®¡è¯é¢‘ï¼Œæ”¯æŒå¿…é¡»è¯ã€é¢‘ç‡è¯ã€è¿‡æ»¤è¯ï¼Œå¹¶æ ‡è®°æ–°å¢æ ‡é¢˜"""

    # å¦‚æœæ²¡æœ‰é…ç½®è¯ç»„ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰æ–°é—»çš„è™šæ‹Ÿè¯ç»„
    if not word_groups:
        print("é¢‘ç‡è¯é…ç½®ä¸ºç©ºï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰æ–°é—»")
        word_groups = [{"required": [], "normal": [], "group_key": "å…¨éƒ¨æ–°é—»"}]
        filter_words = []  # æ¸…ç©ºè¿‡æ»¤è¯ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ–°é—»

    is_first_today = is_first_crawl_today()

    # ç¡®å®šå¤„ç†çš„æ•°æ®æºå’Œæ–°å¢æ ‡è®°é€»è¾‘
    if mode == "incremental":
        if is_first_today:
            # å¢é‡æ¨¡å¼ + å½“å¤©ç¬¬ä¸€æ¬¡ï¼šå¤„ç†æ‰€æœ‰æ–°é—»ï¼Œéƒ½æ ‡è®°ä¸ºæ–°å¢
            results_to_process = results
            all_news_are_new = True
        else:
            # å¢é‡æ¨¡å¼ + å½“å¤©éç¬¬ä¸€æ¬¡ï¼šåªå¤„ç†æ–°å¢çš„æ–°é—»
            results_to_process = new_titles if new_titles else {}
            all_news_are_new = True
    elif mode == "current":
        # current æ¨¡å¼ï¼šåªå¤„ç†å½“å‰æ—¶é—´æ‰¹æ¬¡çš„æ–°é—»ï¼Œä½†ç»Ÿè®¡ä¿¡æ¯æ¥è‡ªå…¨éƒ¨å†å²
        if title_info:
            latest_time = None
            for source_titles in title_info.values():
                for title_data in source_titles.values():
                    last_time = title_data.get("last_time", "")
                    if last_time:
                        if latest_time is None or last_time > latest_time:
                            latest_time = last_time

            # åªå¤„ç† last_time ç­‰äºæœ€æ–°æ—¶é—´çš„æ–°é—»
            if latest_time:
                results_to_process = {}
                for source_id, source_titles in results.items():
                    if source_id in title_info:
                        filtered_titles = {}
                        for title, title_data in source_titles.items():
                            if title in title_info[source_id]:
                                info = title_info[source_id][title]
                                if info.get("last_time") == latest_time:
                                    filtered_titles[title] = title_data
                        if filtered_titles:
                            results_to_process[source_id] = filtered_titles

                print(
                    f"å½“å‰æ¦œå•æ¨¡å¼ï¼šæœ€æ–°æ—¶é—´ {latest_time}ï¼Œç­›é€‰å‡º {sum(len(titles) for titles in results_to_process.values())} æ¡å½“å‰æ¦œå•æ–°é—»"
                )
            else:
                results_to_process = results
        else:
            results_to_process = results
        all_news_are_new = False
    else:
        # å½“æ—¥æ±‡æ€»æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ–°é—»
        results_to_process = results
        all_news_are_new = False
        total_input_news = sum(len(titles) for titles in results.values())
        filter_status = (
            "å…¨éƒ¨æ˜¾ç¤º"
            if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
            else "é¢‘ç‡è¯è¿‡æ»¤"
        )
        print(f"å½“æ—¥æ±‡æ€»æ¨¡å¼ï¼šå¤„ç† {total_input_news} æ¡æ–°é—»ï¼Œæ¨¡å¼ï¼š{filter_status}")

    word_stats = {}
    total_titles = 0
    processed_titles = {}
    matched_new_count = 0

    if title_info is None:
        title_info = {}
    if new_titles is None:
        new_titles = {}

    for group in word_groups:
        group_key = group["group_key"]
        word_stats[group_key] = {"count": 0, "titles": {}}

    for source_id, titles_data in results_to_process.items():
        total_titles += len(titles_data)

        if source_id not in processed_titles:
            processed_titles[source_id] = {}

        for title, title_data in titles_data.items():
            if title in processed_titles.get(source_id, {}):
                continue

            # ä½¿ç”¨ç»Ÿä¸€çš„åŒ¹é…é€»è¾‘
            matches_frequency_words = matches_word_groups(
                title, word_groups, filter_words
            )

            if not matches_frequency_words:
                continue

            # å¦‚æœæ˜¯å¢é‡æ¨¡å¼æˆ– current æ¨¡å¼ç¬¬ä¸€æ¬¡ï¼Œç»Ÿè®¡åŒ¹é…çš„æ–°å¢æ–°é—»æ•°é‡
            if (mode == "incremental" and all_news_are_new) or (
                mode == "current" and is_first_today
            ):
                matched_new_count += 1

            source_ranks = title_data.get("ranks", [])
            source_url = title_data.get("url", "")
            source_mobile_url = title_data.get("mobileUrl", "")

            # æ‰¾åˆ°åŒ¹é…çš„è¯ç»„
            title_lower = title.lower()
            for group in word_groups:
                required_words = group["required"]
                normal_words = group["normal"]

                # å¦‚æœæ˜¯"å…¨éƒ¨æ–°é—»"æ¨¡å¼ï¼Œæ‰€æœ‰æ ‡é¢˜éƒ½åŒ¹é…ç¬¬ä¸€ä¸ªï¼ˆå”¯ä¸€çš„ï¼‰è¯ç»„
                if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»":
                    group_key = group["group_key"]
                    word_stats[group_key]["count"] += 1
                    if source_id not in word_stats[group_key]["titles"]:
                        word_stats[group_key]["titles"][source_id] = []
                else:
                    # åŸæœ‰çš„åŒ¹é…é€»è¾‘
                    if required_words:
                        all_required_present = all(
                            req_word.lower() in title_lower
                            for req_word in required_words
                        )
                        if not all_required_present:
                            continue

                    if normal_words:
                        any_normal_present = any(
                            normal_word.lower() in title_lower
                            for normal_word in normal_words
                        )
                        if not any_normal_present:
                            continue

                    group_key = group["group_key"]
                    word_stats[group_key]["count"] += 1
                    if source_id not in word_stats[group_key]["titles"]:
                        word_stats[group_key]["titles"][source_id] = []

                first_time = ""
                last_time = ""
                count_info = 1
                ranks = source_ranks if source_ranks else []
                url = source_url
                mobile_url = source_mobile_url

                # å¯¹äº current æ¨¡å¼ï¼Œä»å†å²ç»Ÿè®¡ä¿¡æ¯ä¸­è·å–å®Œæ•´æ•°æ®
                if (
                    mode == "current"
                    and title_info
                    and source_id in title_info
                    and title in title_info[source_id]
                ):
                    info = title_info[source_id][title]
                    first_time = info.get("first_time", "")
                    last_time = info.get("last_time", "")
                    count_info = info.get("count", 1)
                    if "ranks" in info and info["ranks"]:
                        ranks = info["ranks"]
                    url = info.get("url", source_url)
                    mobile_url = info.get("mobileUrl", source_mobile_url)
                elif (
                    title_info
                    and source_id in title_info
                    and title in title_info[source_id]
                ):
                    info = title_info[source_id][title]
                    first_time = info.get("first_time", "")
                    last_time = info.get("last_time", "")
                    count_info = info.get("count", 1)
                    if "ranks" in info and info["ranks"]:
                        ranks = info["ranks"]
                    url = info.get("url", source_url)
                    mobile_url = info.get("mobileUrl", source_mobile_url)

                if not ranks:
                    ranks = [99]

                time_display = format_time_display(first_time, last_time)

                source_name = id_to_name.get(source_id, source_id)

                # åˆ¤æ–­æ˜¯å¦ä¸ºæ–°å¢
                is_new = False
                if all_news_are_new:
                    # å¢é‡æ¨¡å¼ä¸‹æ‰€æœ‰å¤„ç†çš„æ–°é—»éƒ½æ˜¯æ–°å¢ï¼Œæˆ–è€…å½“å¤©ç¬¬ä¸€æ¬¡çš„æ‰€æœ‰æ–°é—»éƒ½æ˜¯æ–°å¢
                    is_new = True
                elif new_titles and source_id in new_titles:
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ–°å¢åˆ—è¡¨ä¸­
                    new_titles_for_source = new_titles[source_id]
                    is_new = title in new_titles_for_source

                word_stats[group_key]["titles"][source_id].append(
                    {
                        "title": title,
                        "source_name": source_name,
                        "first_time": first_time,
                        "last_time": last_time,
                        "time_display": time_display,
                        "count": count_info,
                        "ranks": ranks,
                        "rank_threshold": rank_threshold,
                        "url": url,
                        "mobileUrl": mobile_url,
                        "is_new": is_new,
                    }
                )

                if source_id not in processed_titles:
                    processed_titles[source_id] = {}
                processed_titles[source_id][title] = True

                break

    # æœ€åç»Ÿä¸€æ‰“å°æ±‡æ€»ä¿¡æ¯
    if mode == "incremental":
        if is_first_today:
            total_input_news = sum(len(titles) for titles in results.values())
            filter_status = (
                "å…¨éƒ¨æ˜¾ç¤º"
                if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
                else "é¢‘ç‡è¯åŒ¹é…"
            )
            print(
                f"å¢é‡æ¨¡å¼ï¼šå½“å¤©ç¬¬ä¸€æ¬¡çˆ¬å–ï¼Œ{total_input_news} æ¡æ–°é—»ä¸­æœ‰ {matched_new_count} æ¡{filter_status}"
            )
        else:
            if new_titles:
                total_new_count = sum(len(titles) for titles in new_titles.values())
                filter_status = (
                    "å…¨éƒ¨æ˜¾ç¤º"
                    if len(word_groups) == 1
                    and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
                    else "åŒ¹é…é¢‘ç‡è¯"
                )
                print(
                    f"å¢é‡æ¨¡å¼ï¼š{total_new_count} æ¡æ–°å¢æ–°é—»ä¸­ï¼Œæœ‰ {matched_new_count} æ¡{filter_status}"
                )
                if matched_new_count == 0 and len(word_groups) > 1:
                    print("å¢é‡æ¨¡å¼ï¼šæ²¡æœ‰æ–°å¢æ–°é—»åŒ¹é…é¢‘ç‡è¯ï¼Œå°†ä¸ä¼šå‘é€é€šçŸ¥")
            else:
                print("å¢é‡æ¨¡å¼ï¼šæœªæ£€æµ‹åˆ°æ–°å¢æ–°é—»")
    elif mode == "current":
        total_input_news = sum(len(titles) for titles in results_to_process.values())
        if is_first_today:
            filter_status = (
                "å…¨éƒ¨æ˜¾ç¤º"
                if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
                else "é¢‘ç‡è¯åŒ¹é…"
            )
            print(
                f"å½“å‰æ¦œå•æ¨¡å¼ï¼šå½“å¤©ç¬¬ä¸€æ¬¡çˆ¬å–ï¼Œ{total_input_news} æ¡å½“å‰æ¦œå•æ–°é—»ä¸­æœ‰ {matched_new_count} æ¡{filter_status}"
            )
        else:
            matched_count = sum(stat["count"] for stat in word_stats.values())
            filter_status = (
                "å…¨éƒ¨æ˜¾ç¤º"
                if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
                else "é¢‘ç‡è¯åŒ¹é…"
            )
            print(
                f"å½“å‰æ¦œå•æ¨¡å¼ï¼š{total_input_news} æ¡å½“å‰æ¦œå•æ–°é—»ä¸­æœ‰ {matched_count} æ¡{filter_status}"
            )

    stats = []
    for group_key, data in word_stats.items():
        all_titles = []
        for source_id, title_list in data["titles"].items():
            all_titles.extend(title_list)

        # æŒ‰æƒé‡æ’åº
        sorted_titles = sorted(
            all_titles,
            key=lambda x: (
                -calculate_news_weight(x, rank_threshold),
                min(x["ranks"]) if x["ranks"] else 999,
                -x["count"],
            ),
        )

        stats.append(
            {
                "word": group_key,
                "count": data["count"],
                "titles": sorted_titles,
                "percentage": (
                    round(data["count"] / total_titles * 100, 2)
                    if total_titles > 0
                    else 0
                ),
            }
        )

    stats.sort(key=lambda x: x["count"], reverse=True)
    return stats, total_titles


# === æŠ¥å‘Šç”Ÿæˆ ===
def prepare_report_data(
    stats: List[Dict],
    failed_ids: Optional[List] = None,
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    mode: str = "daily",
) -> Dict:
    """å‡†å¤‡æŠ¥å‘Šæ•°æ®"""
    processed_new_titles = []

    # åœ¨å¢é‡æ¨¡å¼ä¸‹éšè—æ–°å¢æ–°é—»åŒºåŸŸ
    hide_new_section = mode == "incremental"

    # åªæœ‰åœ¨ééšè—æ¨¡å¼ä¸‹æ‰å¤„ç†æ–°å¢æ–°é—»éƒ¨åˆ†
    if not hide_new_section:
        filtered_new_titles = {}
        if new_titles and id_to_name:
            word_groups, filter_words = load_frequency_words()
            for source_id, titles_data in new_titles.items():
                filtered_titles = {}
                for title, title_data in titles_data.items():
                    if matches_word_groups(title, word_groups, filter_words):
                        filtered_titles[title] = title_data
                if filtered_titles:
                    filtered_new_titles[source_id] = filtered_titles

        if filtered_new_titles and id_to_name:
            for source_id, titles_data in filtered_new_titles.items():
                source_name = id_to_name.get(source_id, source_id)
                source_titles = []

                for title, title_data in titles_data.items():
                    url = title_data.get("url", "")
                    mobile_url = title_data.get("mobileUrl", "")
                    ranks = title_data.get("ranks", [])

                    processed_title = {
                        "title": title,
                        "source_name": source_name,
                        "time_display": "",
                        "count": 1,
                        "ranks": ranks,
                        "rank_threshold": CONFIG["RANK_THRESHOLD"],
                        "url": url,
                        "mobile_url": mobile_url,
                        "is_new": True,
                    }
                    source_titles.append(processed_title)

                if source_titles:
                    processed_new_titles.append(
                        {
                            "source_id": source_id,
                            "source_name": source_name,
                            "titles": source_titles,
                        }
                    )

    processed_stats = []
    for stat in stats:
        if stat["count"] <= 0:
            continue

        processed_titles = []
        for title_data in stat["titles"]:
            processed_title = {
                "title": title_data["title"],
                "source_name": title_data["source_name"],
                "time_display": title_data["time_display"],
                "count": title_data["count"],
                "ranks": title_data["ranks"],
                "rank_threshold": title_data["rank_threshold"],
                "url": title_data.get("url", ""),
                "mobile_url": title_data.get("mobileUrl", ""),
                "is_new": title_data.get("is_new", False),
            }
            processed_titles.append(processed_title)

        processed_stats.append(
            {
                "word": stat["word"],
                "count": stat["count"],
                "percentage": stat.get("percentage", 0),
                "titles": processed_titles,
            }
        )

    return {
        "stats": processed_stats,
        "new_titles": processed_new_titles,
        "failed_ids": failed_ids or [],
        "total_new_count": sum(
            len(source["titles"]) for source in processed_new_titles
        ),
    }


def format_title_for_platform(
    platform: str, title_data: Dict, show_source: bool = True
) -> str:
    """Format news titles for WeCom notifications or HTML preview"""
    rank_display = format_rank_display(
        title_data["ranks"], title_data["rank_threshold"], platform
    )

    link_url = title_data["mobile_url"] or title_data["url"]
    cleaned_title = clean_title(title_data["title"])

    if platform == "html":
        source_label = (
            f"<span class='source'>{html_escape(title_data['source_name'])}</span>"
            if show_source
            else ""
        )

        title_content = html_escape(cleaned_title)
        if link_url:
            title_content = f"<a href='{html_escape(link_url)}' target='_blank'>{title_content}</a>"

        badges = []
        if title_data.get("is_new"):
            badges.append("<span class='badge badge-new'>NEW</span>")
        if title_data["count"] > 1:
            badges.append(
                f"<span class='badge badge-count'>{title_data['count']}æ¬¡</span>"
            )
        if rank_display:
            badges.append(
                f"<span class='badge badge-rank'>{html_escape(rank_display)}</span>"
            )
        if title_data["time_display"]:
            badges.append(
                f"<span class='badge badge-time'>{html_escape(title_data['time_display'])}</span>"
            )

        badges_html = "".join(badges)
        return f"""
        <div class="news-item{' new' if title_data.get('is_new') else ''}">
            <div class="news-content">
                {source_label}
                <div class="news-title">{title_content}</div>
                <div class="news-meta">{badges_html}</div>
            </div>
        </div>
        """

    if link_url:
        formatted_title = f"[{cleaned_title}]({link_url})"
    else:
        formatted_title = cleaned_title

    title_prefix = "ğŸŒŸ " if title_data.get("is_new") else ""

    if show_source:
        result = f"[{title_data['source_name']}] {title_prefix}{formatted_title}"
    else:
        result = f"{title_prefix}{formatted_title}"

    if rank_display:
        result += f" {rank_display}"
    if title_data["time_display"]:
        result += f" - {title_data['time_display']}"
    if title_data["count"] > 1:
        result += f" ({title_data['count']}æ¬¡)"

    return result



def generate_html_report(
    stats: List[Dict],
    total_titles: int,
    failed_ids: Optional[List] = None,
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    mode: str = "daily",
    is_daily_summary: bool = False,
) -> str:
    """ç”ŸæˆHTMLæŠ¥å‘Š"""
    if is_daily_summary:
        if mode == "current":
            filename = "å½“å‰æ¦œå•æ±‡æ€».html"
        elif mode == "incremental":
            filename = "å½“æ—¥å¢é‡.html"
        else:
            filename = "å½“æ—¥æ±‡æ€».html"
    else:
        filename = f"{format_time_filename()}.html"

    file_path = get_output_path("html", filename)

    report_data = prepare_report_data(stats, failed_ids, new_titles, id_to_name, mode)

    html_content = render_html_content(
        report_data, total_titles, is_daily_summary, mode
    )

    with open(file_path, "w", encoding="utf-8") as f:
        f.write(html_content)

    if is_daily_summary:
        root_file_path = Path("index.html")
        with open(root_file_path, "w", encoding="utf-8") as f:
            f.write(html_content)

    return file_path


def render_html_content(
    report_data: Dict,
    total_titles: int,
    is_daily_summary: bool = False,
    mode: str = "daily",
) -> str:
    """æ¸²æŸ“HTMLå†…å®¹"""
    html = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>çƒ­ç‚¹æ–°é—»åˆ†æ</title>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.4.1/html2canvas.min.js" integrity="sha512-BNaRQnYJYiPSqHHDb58B0yaPfCu+Wgds8Gp/gU33kqBtgNS4tSPHuGibyoeqMV/TJlSKda6FXzoEyYGjTe+vXA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
        <style>
            * { box-sizing: border-box; }
            body { 
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
                margin: 0; 
                padding: 16px; 
                background: #fafafa;
                color: #333;
                line-height: 1.5;
            }
            
            .container {
                max-width: 600px;
                margin: 0 auto;
                background: white;
                border-radius: 12px;
                overflow: hidden;
                box-shadow: 0 2px 16px rgba(0,0,0,0.06);
            }
            
            .header {
                background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%);
                color: white;
                padding: 32px 24px;
                text-align: center;
                position: relative;
            }
            
            .save-buttons {
                position: absolute;
                top: 16px;
                right: 16px;
                display: flex;
                gap: 8px;
            }
            
            .save-btn {
                background: rgba(255, 255, 255, 0.2);
                border: 1px solid rgba(255, 255, 255, 0.3);
                color: white;
                padding: 8px 16px;
                border-radius: 6px;
                cursor: pointer;
                font-size: 13px;
                font-weight: 500;
                transition: all 0.2s ease;
                backdrop-filter: blur(10px);
                white-space: nowrap;
            }
            
            .save-btn:hover {
                background: rgba(255, 255, 255, 0.3);
                border-color: rgba(255, 255, 255, 0.5);
                transform: translateY(-1px);
            }
            
            .save-btn:active {
                transform: translateY(0);
            }
            
            .save-btn:disabled {
                opacity: 0.6;
                cursor: not-allowed;
            }
            
            .header-title {
                font-size: 22px;
                font-weight: 700;
                margin: 0 0 20px 0;
            }
            
            .header-info {
                display: grid;
                grid-template-columns: 1fr 1fr;
                gap: 16px;
                font-size: 14px;
                opacity: 0.95;
            }
            
            .info-item {
                text-align: center;
            }
            
            .info-label {
                display: block;
                font-size: 12px;
                opacity: 0.8;
                margin-bottom: 4px;
            }
            
            .info-value {
                font-weight: 600;
                font-size: 16px;
            }
            
            .content {
                padding: 24px;
            }
            
            .word-group {
                margin-bottom: 40px;
            }
            
            .word-group:first-child {
                margin-top: 0;
            }
            
            .word-header {
                display: flex;
                align-items: center;
                justify-content: space-between;
                margin-bottom: 20px;
                padding-bottom: 8px;
                border-bottom: 1px solid #f0f0f0;
            }
            
            .word-info {
                display: flex;
                align-items: center;
                gap: 12px;
            }
            
            .word-name {
                font-size: 17px;
                font-weight: 600;
                color: #1a1a1a;
            }
            
            .word-count {
                color: #666;
                font-size: 13px;
                font-weight: 500;
            }
            
            .word-count.hot { color: #dc2626; font-weight: 600; }
            .word-count.warm { color: #ea580c; font-weight: 600; }
            
            .word-index {
                color: #999;
                font-size: 12px;
            }
            
            .news-item {
                margin-bottom: 20px;
                padding: 16px 0;
                border-bottom: 1px solid #f5f5f5;
                position: relative;
                display: flex;
                gap: 12px;
                align-items: center;
            }
            
            .news-item:last-child {
                border-bottom: none;
            }
            
            .news-item.new::after {
                content: "NEW";
                position: absolute;
                top: 12px;
                right: 0;
                background: #fbbf24;
                color: #92400e;
                font-size: 9px;
                font-weight: 700;
                padding: 3px 6px;
                border-radius: 4px;
                letter-spacing: 0.5px;
            }
            
            .news-number {
                color: #999;
                font-size: 13px;
                font-weight: 600;
                min-width: 20px;
                text-align: center;
                flex-shrink: 0;
                background: #f8f9fa;
                border-radius: 50%;
                width: 24px;
                height: 24px;
                display: flex;
                align-items: center;
                justify-content: center;
                align-self: flex-start;
                margin-top: 8px;
            }
            
            .news-content {
                flex: 1;
                min-width: 0;
                padding-right: 40px;
            }
            
            .news-item.new .news-content {
                padding-right: 50px;
            }
            
            .news-header {
                display: flex;
                align-items: center;
                gap: 8px;
                margin-bottom: 8px;
                flex-wrap: wrap;
            }
            
            .source-name {
                color: #666;
                font-size: 12px;
                font-weight: 500;
            }
            
            .rank-num {
                color: #fff;
                background: #6b7280;
                font-size: 10px;
                font-weight: 700;
                padding: 2px 6px;
                border-radius: 10px;
                min-width: 18px;
                text-align: center;
            }
            
            .rank-num.top { background: #dc2626; }
            .rank-num.high { background: #ea580c; }
            
            .time-info {
                color: #999;
                font-size: 11px;
            }
            
            .count-info {
                color: #059669;
                font-size: 11px;
                font-weight: 500;
            }
            
            .news-title {
                font-size: 15px;
                line-height: 1.4;
                color: #1a1a1a;
                margin: 0;
            }
            
            .news-link {
                color: #2563eb;
                text-decoration: none;
            }
            
            .news-link:hover {
                text-decoration: underline;
            }
            
            .news-link:visited {
                color: #7c3aed;
            }
            
            .new-section {
                margin-top: 40px;
                padding-top: 24px;
                border-top: 2px solid #f0f0f0;
            }
            
            .new-section-title {
                color: #1a1a1a;
                font-size: 16px;
                font-weight: 600;
                margin: 0 0 20px 0;
            }
            
            .new-source-group {
                margin-bottom: 24px;
            }
            
            .new-source-title {
                color: #666;
                font-size: 13px;
                font-weight: 500;
                margin: 0 0 12px 0;
                padding-bottom: 6px;
                border-bottom: 1px solid #f5f5f5;
            }
            
            .new-item {
                display: flex;
                align-items: center;
                gap: 12px;
                padding: 8px 0;
                border-bottom: 1px solid #f9f9f9;
            }
            
            .new-item:last-child {
                border-bottom: none;
            }
            
            .new-item-number {
                color: #999;
                font-size: 12px;
                font-weight: 600;
                min-width: 18px;
                text-align: center;
                flex-shrink: 0;
                background: #f8f9fa;
                border-radius: 50%;
                width: 20px;
                height: 20px;
                display: flex;
                align-items: center;
                justify-content: center;
            }
            
            .new-item-rank {
                color: #fff;
                background: #6b7280;
                font-size: 10px;
                font-weight: 700;
                padding: 3px 6px;
                border-radius: 8px;
                min-width: 20px;
                text-align: center;
                flex-shrink: 0;
            }
            
            .new-item-rank.top { background: #dc2626; }
            .new-item-rank.high { background: #ea580c; }
            
            .new-item-content {
                flex: 1;
                min-width: 0;
            }
            
            .new-item-title {
                font-size: 14px;
                line-height: 1.4;
                color: #1a1a1a;
                margin: 0;
            }
            
            .error-section {
                background: #fef2f2;
                border: 1px solid #fecaca;
                border-radius: 8px;
                padding: 16px;
                margin-bottom: 24px;
            }
            
            .error-title {
                color: #dc2626;
                font-size: 14px;
                font-weight: 600;
                margin: 0 0 8px 0;
            }
            
            .error-list {
                list-style: none;
                padding: 0;
                margin: 0;
            }
            
            .error-item {
                color: #991b1b;
                font-size: 13px;
                padding: 2px 0;
                font-family: 'SF Mono', Consolas, monospace;
            }
            
            .footer {
                margin-top: 32px;
                padding: 20px 24px;
                background: #f8f9fa;
                border-top: 1px solid #e5e7eb;
                text-align: center;
            }
            
            .footer-content {
                font-size: 13px;
                color: #6b7280;
                line-height: 1.6;
            }
            
            .footer-link {
                color: #4f46e5;
                text-decoration: none;
                font-weight: 500;
                transition: color 0.2s ease;
            }
            
            .footer-link:hover {
                color: #7c3aed;
                text-decoration: underline;
            }
            
            .project-name {
                font-weight: 600;
                color: #374151;
            }
            
            @media (max-width: 480px) {
                body { padding: 12px; }
                .header { padding: 24px 20px; }
                .content { padding: 20px; }
                .footer { padding: 16px 20px; }
                .header-info { grid-template-columns: 1fr; gap: 12px; }
                .news-header { gap: 6px; }
                .news-content { padding-right: 45px; }
                .news-item { gap: 8px; }
                .new-item { gap: 8px; }
                .news-number { width: 20px; height: 20px; font-size: 12px; }
                .save-buttons {
                    position: static;
                    margin-bottom: 16px;
                    display: flex;
                    gap: 8px;
                    justify-content: center;
                    flex-direction: column;
                    width: 100%;
                }
                .save-btn {
                    width: 100%;
                }
            }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <div class="save-buttons">
                    <button class="save-btn" onclick="saveAsImage()">ä¿å­˜ä¸ºå›¾ç‰‡</button>
                    <button class="save-btn" onclick="saveAsMultipleImages()">åˆ†æ®µä¿å­˜</button>
                </div>
                <div class="header-title">çƒ­ç‚¹æ–°é—»åˆ†æ</div>
                <div class="header-info">
                    <div class="info-item">
                        <span class="info-label">æŠ¥å‘Šç±»å‹</span>
                        <span class="info-value">"""

    # å¤„ç†æŠ¥å‘Šç±»å‹æ˜¾ç¤º
    if is_daily_summary:
        if mode == "current":
            html += "å½“å‰æ¦œå•"
        elif mode == "incremental":
            html += "å¢é‡æ¨¡å¼"
        else:
            html += "å½“æ—¥æ±‡æ€»"
    else:
        html += "å®æ—¶åˆ†æ"

    html += """</span>
                    </div>
                    <div class="info-item">
                        <span class="info-label">æ–°é—»æ€»æ•°</span>
                        <span class="info-value">"""

    html += f"{total_titles} æ¡"

    # è®¡ç®—ç­›é€‰åçš„çƒ­ç‚¹æ–°é—»æ•°é‡
    hot_news_count = sum(len(stat["titles"]) for stat in report_data["stats"])

    html += """</span>
                    </div>
                    <div class="info-item">
                        <span class="info-label">çƒ­ç‚¹æ–°é—»</span>
                        <span class="info-value">"""

    html += f"{hot_news_count} æ¡"

    html += """</span>
                    </div>
                    <div class="info-item">
                        <span class="info-label">ç”Ÿæˆæ—¶é—´</span>
                        <span class="info-value">"""

    now = get_beijing_time()
    html += now.strftime("%m-%d %H:%M")

    html += """</span>
                    </div>
                </div>
            </div>
            
            <div class="content">"""

    # å¤„ç†å¤±è´¥IDé”™è¯¯ä¿¡æ¯
    if report_data["failed_ids"]:
        html += """
                <div class="error-section">
                    <div class="error-title">âš ï¸ è¯·æ±‚å¤±è´¥çš„å¹³å°</div>
                    <ul class="error-list">"""
        for id_value in report_data["failed_ids"]:
            html += f'<li class="error-item">{html_escape(id_value)}</li>'
        html += """
                    </ul>
                </div>"""

    # å¤„ç†ä¸»è¦ç»Ÿè®¡æ•°æ®
    if report_data["stats"]:
        total_count = len(report_data["stats"])

        for i, stat in enumerate(report_data["stats"], 1):
            count = stat["count"]

            # ç¡®å®šçƒ­åº¦ç­‰çº§
            if count >= 10:
                count_class = "hot"
            elif count >= 5:
                count_class = "warm"
            else:
                count_class = ""

            escaped_word = html_escape(stat["word"])

            html += f"""
                <div class="word-group">
                    <div class="word-header">
                        <div class="word-info">
                            <div class="word-name">{escaped_word}</div>
                            <div class="word-count {count_class}">{count} æ¡</div>
                        </div>
                        <div class="word-index">{i}/{total_count}</div>
                    </div>"""

            # å¤„ç†æ¯ä¸ªè¯ç»„ä¸‹çš„æ–°é—»æ ‡é¢˜ï¼Œç»™æ¯æ¡æ–°é—»æ ‡ä¸Šåºå·
            for j, title_data in enumerate(stat["titles"], 1):
                is_new = title_data.get("is_new", False)
                new_class = "new" if is_new else ""

                html += f"""
                    <div class="news-item {new_class}">
                        <div class="news-number">{j}</div>
                        <div class="news-content">
                            <div class="news-header">
                                <span class="source-name">{html_escape(title_data["source_name"])}</span>"""

                # å¤„ç†æ’åæ˜¾ç¤º
                ranks = title_data.get("ranks", [])
                if ranks:
                    min_rank = min(ranks)
                    max_rank = max(ranks)
                    rank_threshold = title_data.get("rank_threshold", 10)

                    # ç¡®å®šæ’åç­‰çº§
                    if min_rank <= 3:
                        rank_class = "top"
                    elif min_rank <= rank_threshold:
                        rank_class = "high"
                    else:
                        rank_class = ""

                    if min_rank == max_rank:
                        rank_text = str(min_rank)
                    else:
                        rank_text = f"{min_rank}-{max_rank}"

                    html += f'<span class="rank-num {rank_class}">{rank_text}</span>'

                # å¤„ç†æ—¶é—´æ˜¾ç¤º
                time_display = title_data.get("time_display", "")
                if time_display:
                    # ç®€åŒ–æ—¶é—´æ˜¾ç¤ºæ ¼å¼ï¼Œå°†æ³¢æµªçº¿æ›¿æ¢ä¸º~
                    simplified_time = (
                        time_display.replace(" ~ ", "~")
                        .replace("[", "")
                        .replace("]", "")
                    )
                    html += (
                        f'<span class="time-info">{html_escape(simplified_time)}</span>'
                    )

                # å¤„ç†å‡ºç°æ¬¡æ•°
                count_info = title_data.get("count", 1)
                if count_info > 1:
                    html += f'<span class="count-info">{count_info}æ¬¡</span>'

                html += """
                            </div>
                            <div class="news-title">"""

                # å¤„ç†æ ‡é¢˜å’Œé“¾æ¥
                escaped_title = html_escape(title_data["title"])
                link_url = title_data.get("mobile_url") or title_data.get("url", "")

                if link_url:
                    escaped_url = html_escape(link_url)
                    html += f'<a href="{escaped_url}" target="_blank" class="news-link">{escaped_title}</a>'
                else:
                    html += escaped_title

                html += """
                            </div>
                        </div>
                    </div>"""

            html += """
                </div>"""

    # å¤„ç†æ–°å¢æ–°é—»åŒºåŸŸ
    if report_data["new_titles"]:
        html += f"""
                <div class="new-section">
                    <div class="new-section-title">æœ¬æ¬¡æ–°å¢çƒ­ç‚¹ (å…± {report_data['total_new_count']} æ¡)</div>"""

        for source_data in report_data["new_titles"]:
            escaped_source = html_escape(source_data["source_name"])
            titles_count = len(source_data["titles"])

            html += f"""
                    <div class="new-source-group">
                        <div class="new-source-title">{escaped_source} Â· {titles_count}æ¡</div>"""

            # ä¸ºæ–°å¢æ–°é—»ä¹Ÿæ·»åŠ åºå·
            for idx, title_data in enumerate(source_data["titles"], 1):
                ranks = title_data.get("ranks", [])

                # å¤„ç†æ–°å¢æ–°é—»çš„æ’åæ˜¾ç¤º
                rank_class = ""
                if ranks:
                    min_rank = min(ranks)
                    if min_rank <= 3:
                        rank_class = "top"
                    elif min_rank <= title_data.get("rank_threshold", 10):
                        rank_class = "high"

                    if len(ranks) == 1:
                        rank_text = str(ranks[0])
                    else:
                        rank_text = f"{min(ranks)}-{max(ranks)}"
                else:
                    rank_text = "?"

                html += f"""
                        <div class="new-item">
                            <div class="new-item-number">{idx}</div>
                            <div class="new-item-rank {rank_class}">{rank_text}</div>
                            <div class="new-item-content">
                                <div class="new-item-title">"""

                # å¤„ç†æ–°å¢æ–°é—»çš„é“¾æ¥
                escaped_title = html_escape(title_data["title"])
                link_url = title_data.get("mobile_url") or title_data.get("url", "")

                if link_url:
                    escaped_url = html_escape(link_url)
                    html += f'<a href="{escaped_url}" target="_blank" class="news-link">{escaped_title}</a>'
                else:
                    html += escaped_title

                html += """
                                </div>
                            </div>
                        </div>"""

            html += """
                    </div>"""

        html += """
                </div>"""

    html += """
            </div>
            
            <div class="footer">
                <div class="footer-content">
                    ç”± <span class="project-name">TrendRadar</span> ç”Ÿæˆ Â· 
                    <a href="https://github.com/sansan0/TrendRadar" target="_blank" class="footer-link">
                        GitHub å¼€æºé¡¹ç›®
                    </a>"""

    html += """
                </div>
            </div>
        </div>
        
        <script>
            async function saveAsImage() {
                const button = event.target;
                const originalText = button.textContent;
                
                try {
                    button.textContent = 'ç”Ÿæˆä¸­...';
                    button.disabled = true;
                    window.scrollTo(0, 0);
                    
                    // ç­‰å¾…é¡µé¢ç¨³å®š
                    await new Promise(resolve => setTimeout(resolve, 200));
                    
                    // æˆªå›¾å‰éšè—æŒ‰é’®
                    const buttons = document.querySelector('.save-buttons');
                    buttons.style.visibility = 'hidden';
                    
                    // å†æ¬¡ç­‰å¾…ç¡®ä¿æŒ‰é’®å®Œå…¨éšè—
                    await new Promise(resolve => setTimeout(resolve, 100));
                    
                    const container = document.querySelector('.container');
                    
                    const canvas = await html2canvas(container, {
                        backgroundColor: '#ffffff',
                        scale: 1.5,
                        useCORS: true,
                        allowTaint: false,
                        imageTimeout: 10000,
                        removeContainer: false,
                        foreignObjectRendering: false,
                        logging: false,
                        width: container.offsetWidth,
                        height: container.offsetHeight,
                        x: 0,
                        y: 0,
                        scrollX: 0,
                        scrollY: 0,
                        windowWidth: window.innerWidth,
                        windowHeight: window.innerHeight
                    });
                    
                    buttons.style.visibility = 'visible';
                    
                    const link = document.createElement('a');
                    const now = new Date();
                    const filename = `TrendRadar_çƒ­ç‚¹æ–°é—»åˆ†æ_${now.getFullYear()}${String(now.getMonth() + 1).padStart(2, '0')}${String(now.getDate()).padStart(2, '0')}_${String(now.getHours()).padStart(2, '0')}${String(now.getMinutes()).padStart(2, '0')}.png`;
                    
                    link.download = filename;
                    link.href = canvas.toDataURL('image/png', 1.0);
                    
                    // è§¦å‘ä¸‹è½½
                    document.body.appendChild(link);
                    link.click();
                    document.body.removeChild(link);
                    
                    button.textContent = 'ä¿å­˜æˆåŠŸ!';
                    setTimeout(() => {
                        button.textContent = originalText;
                        button.disabled = false;
                    }, 2000);
                    
                } catch (error) {
                    const buttons = document.querySelector('.save-buttons');
                    buttons.style.visibility = 'visible';
                    button.textContent = 'ä¿å­˜å¤±è´¥';
                    setTimeout(() => {
                        button.textContent = originalText;
                        button.disabled = false;
                    }, 2000);
                }
            }
            
            async function saveAsMultipleImages() {
                const button = event.target;
                const originalText = button.textContent;
                const container = document.querySelector('.container');
                const scale = 1.5; 
                const maxHeight = 5000 / scale;
                
                try {
                    button.textContent = 'åˆ†æä¸­...';
                    button.disabled = true;
                    
                    // è·å–æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²å…ƒç´ 
                    const newsItems = Array.from(container.querySelectorAll('.news-item'));
                    const wordGroups = Array.from(container.querySelectorAll('.word-group'));
                    const newSection = container.querySelector('.new-section');
                    const errorSection = container.querySelector('.error-section');
                    const header = container.querySelector('.header');
                    const footer = container.querySelector('.footer');
                    
                    // è®¡ç®—å…ƒç´ ä½ç½®å’Œé«˜åº¦
                    const containerRect = container.getBoundingClientRect();
                    const elements = [];
                    
                    // æ·»åŠ headerä½œä¸ºå¿…é¡»åŒ…å«çš„å…ƒç´ 
                    elements.push({
                        type: 'header',
                        element: header,
                        top: 0,
                        bottom: header.offsetHeight,
                        height: header.offsetHeight
                    });
                    
                    // æ·»åŠ é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if (errorSection) {
                        const rect = errorSection.getBoundingClientRect();
                        elements.push({
                            type: 'error',
                            element: errorSection,
                            top: rect.top - containerRect.top,
                            bottom: rect.bottom - containerRect.top,
                            height: rect.height
                        });
                    }
                    
                    // æŒ‰word-groupåˆ†ç»„å¤„ç†news-item
                    wordGroups.forEach(group => {
                        const groupRect = group.getBoundingClientRect();
                        const groupNewsItems = group.querySelectorAll('.news-item');
                        
                        // æ·»åŠ word-groupçš„headeréƒ¨åˆ†
                        const wordHeader = group.querySelector('.word-header');
                        if (wordHeader) {
                            const headerRect = wordHeader.getBoundingClientRect();
                            elements.push({
                                type: 'word-header',
                                element: wordHeader,
                                parent: group,
                                top: groupRect.top - containerRect.top,
                                bottom: headerRect.bottom - containerRect.top,
                                height: headerRect.height
                            });
                        }
                        
                        // æ·»åŠ æ¯ä¸ªnews-item
                        groupNewsItems.forEach(item => {
                            const rect = item.getBoundingClientRect();
                            elements.push({
                                type: 'news-item',
                                element: item,
                                parent: group,
                                top: rect.top - containerRect.top,
                                bottom: rect.bottom - containerRect.top,
                                height: rect.height
                            });
                        });
                    });
                    
                    // æ·»åŠ æ–°å¢æ–°é—»éƒ¨åˆ†
                    if (newSection) {
                        const rect = newSection.getBoundingClientRect();
                        elements.push({
                            type: 'new-section',
                            element: newSection,
                            top: rect.top - containerRect.top,
                            bottom: rect.bottom - containerRect.top,
                            height: rect.height
                        });
                    }
                    
                    // æ·»åŠ footer
                    const footerRect = footer.getBoundingClientRect();
                    elements.push({
                        type: 'footer',
                        element: footer,
                        top: footerRect.top - containerRect.top,
                        bottom: footerRect.bottom - containerRect.top,
                        height: footer.offsetHeight
                    });
                    
                    // è®¡ç®—åˆ†å‰²ç‚¹
                    const segments = [];
                    let currentSegment = { start: 0, end: 0, height: 0, includeHeader: true };
                    let headerHeight = header.offsetHeight;
                    currentSegment.height = headerHeight;
                    
                    for (let i = 1; i < elements.length; i++) {
                        const element = elements[i];
                        const potentialHeight = element.bottom - currentSegment.start;
                        
                        // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°åˆ†æ®µ
                        if (potentialHeight > maxHeight && currentSegment.height > headerHeight) {
                            // åœ¨å‰ä¸€ä¸ªå…ƒç´ ç»“æŸå¤„åˆ†å‰²
                            currentSegment.end = elements[i - 1].bottom;
                            segments.push(currentSegment);
                            
                            // å¼€å§‹æ–°åˆ†æ®µ
                            currentSegment = {
                                start: currentSegment.end,
                                end: 0,
                                height: element.bottom - currentSegment.end,
                                includeHeader: false
                            };
                        } else {
                            currentSegment.height = potentialHeight;
                            currentSegment.end = element.bottom;
                        }
                    }
                    
                    // æ·»åŠ æœ€åä¸€ä¸ªåˆ†æ®µ
                    if (currentSegment.height > 0) {
                        currentSegment.end = container.offsetHeight;
                        segments.push(currentSegment);
                    }
                    
                    button.textContent = `ç”Ÿæˆä¸­ (0/${segments.length})...`;
                    
                    // éšè—ä¿å­˜æŒ‰é’®
                    const buttons = document.querySelector('.save-buttons');
                    buttons.style.visibility = 'hidden';
                    
                    // ä¸ºæ¯ä¸ªåˆ†æ®µç”Ÿæˆå›¾ç‰‡
                    const images = [];
                    for (let i = 0; i < segments.length; i++) {
                        const segment = segments[i];
                        button.textContent = `ç”Ÿæˆä¸­ (${i + 1}/${segments.length})...`;
                        
                        // åˆ›å»ºä¸´æ—¶å®¹å™¨ç”¨äºæˆªå›¾
                        const tempContainer = document.createElement('div');
                        tempContainer.style.cssText = `
                            position: absolute;
                            left: -9999px;
                            top: 0;
                            width: ${container.offsetWidth}px;
                            background: white;
                        `;
                        tempContainer.className = 'container';
                        
                        // å…‹éš†å®¹å™¨å†…å®¹
                        const clonedContainer = container.cloneNode(true);
                        
                        // ç§»é™¤å…‹éš†å†…å®¹ä¸­çš„ä¿å­˜æŒ‰é’®
                        const clonedButtons = clonedContainer.querySelector('.save-buttons');
                        if (clonedButtons) {
                            clonedButtons.style.display = 'none';
                        }
                        
                        tempContainer.appendChild(clonedContainer);
                        document.body.appendChild(tempContainer);
                        
                        // ç­‰å¾…DOMæ›´æ–°
                        await new Promise(resolve => setTimeout(resolve, 100));
                        
                        // ä½¿ç”¨html2canvasæˆªå–ç‰¹å®šåŒºåŸŸ
                        const canvas = await html2canvas(clonedContainer, {
                            backgroundColor: '#ffffff',
                            scale: scale,
                            useCORS: true,
                            allowTaint: false,
                            imageTimeout: 10000,
                            logging: false,
                            width: container.offsetWidth,
                            height: segment.end - segment.start,
                            x: 0,
                            y: segment.start,
                            windowWidth: window.innerWidth,
                            windowHeight: window.innerHeight
                        });
                        
                        images.push(canvas.toDataURL('image/png', 1.0));
                        
                        // æ¸…ç†ä¸´æ—¶å®¹å™¨
                        document.body.removeChild(tempContainer);
                    }
                    
                    // æ¢å¤æŒ‰é’®æ˜¾ç¤º
                    buttons.style.visibility = 'visible';
                    
                    // ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
                    const now = new Date();
                    const baseFilename = `TrendRadar_çƒ­ç‚¹æ–°é—»åˆ†æ_${now.getFullYear()}${String(now.getMonth() + 1).padStart(2, '0')}${String(now.getDate()).padStart(2, '0')}_${String(now.getHours()).padStart(2, '0')}${String(now.getMinutes()).padStart(2, '0')}`;
                    
                    for (let i = 0; i < images.length; i++) {
                        const link = document.createElement('a');
                        link.download = `${baseFilename}_part${i + 1}.png`;
                        link.href = images[i];
                        document.body.appendChild(link);
                        link.click();
                        document.body.removeChild(link);
                        
                        // å»¶è¿Ÿä¸€ä¸‹é¿å…æµè§ˆå™¨é˜»æ­¢å¤šä¸ªä¸‹è½½
                        await new Promise(resolve => setTimeout(resolve, 100));
                    }
                    
                    button.textContent = `å·²ä¿å­˜ ${segments.length} å¼ å›¾ç‰‡!`;
                    setTimeout(() => {
                        button.textContent = originalText;
                        button.disabled = false;
                    }, 2000);
                    
                } catch (error) {
                    console.error('åˆ†æ®µä¿å­˜å¤±è´¥:', error);
                    const buttons = document.querySelector('.save-buttons');
                    buttons.style.visibility = 'visible';
                    button.textContent = 'ä¿å­˜å¤±è´¥';
                    setTimeout(() => {
                        button.textContent = originalText;
                        button.disabled = false;
                    }, 2000);
                }
            }
            
            document.addEventListener('DOMContentLoaded', function() {
                window.scrollTo(0, 0);
            });
        </script>
    </body>
    </html>
    """

    return html


def split_content_into_batches(
    report_data: Dict,
    format_type: str,
    max_bytes: int = None,
    mode: str = "daily",
) -> List[str]:
    if format_type != "wework":
        raise ValueError("ä»…æ”¯æŒä¼ä¸šå¾®ä¿¡æ¸ é“è¾“å‡º")
    """åˆ†æ‰¹å¤„ç†æ¶ˆæ¯å†…å®¹ï¼Œç¡®ä¿è¯ç»„æ ‡é¢˜+è‡³å°‘ç¬¬ä¸€æ¡æ–°é—»çš„å®Œæ•´æ€§"""
    if max_bytes is None:
        if format_type == "dingtalk":
            max_bytes = CONFIG.get("DINGTALK_BATCH_SIZE", 20000)
        elif format_type == "feishu":
            max_bytes = CONFIG.get("FEISHU_BATCH_SIZE", 29000)
        elif format_type == "ntfy":
            max_bytes = 3800
        else:
            max_bytes = CONFIG.get("MESSAGE_BATCH_SIZE", 4000)

    batches = []

    total_titles = sum(
        len(stat["titles"]) for stat in report_data["stats"] if stat["count"] > 0
    )
    now = get_beijing_time()

    base_header = ""
    if format_type == "wework":
        base_header = f"**æ€»æ–°é—»æ•°ï¼š** {total_titles}\n\n\n\n"
    elif format_type == "telegram":
        base_header = f"æ€»æ–°é—»æ•°ï¼š {total_titles}\n\n"
    elif format_type == "ntfy":
        base_header = f"**æ€»æ–°é—»æ•°ï¼š** {total_titles}\n\n"
    elif format_type == "feishu":
        base_header = ""
    elif format_type == "dingtalk":
        base_header = f"**æ€»æ–°é—»æ•°ï¼š** {total_titles}\n\n"
        base_header += f"**æ—¶é—´ï¼š** {now.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        base_header += f"**ç±»å‹ï¼š** çƒ­ç‚¹åˆ†ææŠ¥å‘Š\n\n"
        base_header += "---\n\n"

    base_footer = ""
    if format_type == "wework":
        base_footer = f"\n\n\n> æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"
    elif format_type == "telegram":
        base_footer = f"\n\næ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"
    elif format_type == "ntfy":
        base_footer = f"\n\n> æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"
    elif format_type == "feishu":
        base_footer = f"\n\n<font color='grey'>æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}</font>"
    elif format_type == "dingtalk":
        base_footer = f"\n\n> æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"

    stats_header = ""
    if report_data["stats"]:
        if format_type == "wework":
            stats_header = f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"
        elif format_type == "telegram":
            stats_header = f"ğŸ“Š çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡\n\n"
        elif format_type == "ntfy":
            stats_header = f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"
        elif format_type == "feishu":
            stats_header = f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"
        elif format_type == "dingtalk":
            stats_header = f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"

    current_batch = base_header
    current_batch_has_content = False

    if (
        not report_data["stats"]
        and not report_data["new_titles"]
        and not report_data["failed_ids"]
    ):
        if mode == "incremental":
            mode_text = "å¢é‡æ¨¡å¼ä¸‹æš‚æ— æ–°å¢åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        elif mode == "current":
            mode_text = "å½“å‰æ¦œå•æ¨¡å¼ä¸‹æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        else:
            mode_text = "æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        simple_content = f"ğŸ“­ {mode_text}\n\n"
        final_content = base_header + simple_content + base_footer
        batches.append(final_content)
        return batches

    # å¤„ç†çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡
    if report_data["stats"]:
        total_count = len(report_data["stats"])

        # æ·»åŠ ç»Ÿè®¡æ ‡é¢˜
        test_content = current_batch + stats_header
        if (
            len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
            < max_bytes
        ):
            current_batch = test_content
            current_batch_has_content = True
        else:
            if current_batch_has_content:
                batches.append(current_batch + base_footer)
            current_batch = base_header + stats_header
            current_batch_has_content = True

        # é€ä¸ªå¤„ç†è¯ç»„ï¼ˆç¡®ä¿è¯ç»„æ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»çš„åŸå­æ€§ï¼‰
        for i, stat in enumerate(report_data["stats"]):
            word = stat["word"]
            count = stat["count"]
            sequence_display = f"[{i + 1}/{total_count}]"

            # æ„å»ºè¯ç»„æ ‡é¢˜
            word_header = ""
            if format_type == "wework":
                if count >= 10:
                    word_header = (
                        f"ğŸ”¥ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                elif count >= 5:
                    word_header = (
                        f"ğŸ“ˆ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                else:
                    word_header = f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"
            elif format_type == "telegram":
                if count >= 10:
                    word_header = f"ğŸ”¥ {sequence_display} {word} : {count} æ¡\n\n"
                elif count >= 5:
                    word_header = f"ğŸ“ˆ {sequence_display} {word} : {count} æ¡\n\n"
                else:
                    word_header = f"ğŸ“Œ {sequence_display} {word} : {count} æ¡\n\n"
            elif format_type == "ntfy":
                if count >= 10:
                    word_header = (
                        f"ğŸ”¥ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                elif count >= 5:
                    word_header = (
                        f"ğŸ“ˆ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                else:
                    word_header = f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"
            elif format_type == "feishu":
                if count >= 10:
                    word_header = f"ğŸ”¥ <font color='grey'>{sequence_display}</font> **{word}** : <font color='red'>{count}</font> æ¡\n\n"
                elif count >= 5:
                    word_header = f"ğŸ“ˆ <font color='grey'>{sequence_display}</font> **{word}** : <font color='orange'>{count}</font> æ¡\n\n"
                else:
                    word_header = f"ğŸ“Œ <font color='grey'>{sequence_display}</font> **{word}** : {count} æ¡\n\n"
            elif format_type == "dingtalk":
                if count >= 10:
                    word_header = (
                        f"ğŸ”¥ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                elif count >= 5:
                    word_header = (
                        f"ğŸ“ˆ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                else:
                    word_header = f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"

            # æ„å»ºç¬¬ä¸€æ¡æ–°é—»
            first_news_line = ""
            if stat["titles"]:
                first_title_data = stat["titles"][0]
                if format_type == "wework":
                    formatted_title = format_title_for_platform(
                        "wework", first_title_data, show_source=True
                    )
                elif format_type == "telegram":
                    formatted_title = format_title_for_platform(
                        "telegram", first_title_data, show_source=True
                    )
                elif format_type == "ntfy":
                    formatted_title = format_title_for_platform(
                        "ntfy", first_title_data, show_source=True
                    )
                elif format_type == "feishu":
                    formatted_title = format_title_for_platform(
                        "feishu", first_title_data, show_source=True
                    )
                elif format_type == "dingtalk":
                    formatted_title = format_title_for_platform(
                        "dingtalk", first_title_data, show_source=True
                    )
                else:
                    formatted_title = f"{first_title_data['title']}"

                first_news_line = f"  1. {formatted_title}\n"
                if len(stat["titles"]) > 1:
                    first_news_line += "\n"

            # åŸå­æ€§æ£€æŸ¥ï¼šè¯ç»„æ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»å¿…é¡»ä¸€èµ·å¤„ç†
            word_with_first_news = word_header + first_news_line
            test_content = current_batch + word_with_first_news

            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                >= max_bytes
            ):
                # å½“å‰æ‰¹æ¬¡å®¹çº³ä¸ä¸‹ï¼Œå¼€å¯æ–°æ‰¹æ¬¡
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + stats_header + word_with_first_news
                current_batch_has_content = True
                start_index = 1
            else:
                current_batch = test_content
                current_batch_has_content = True
                start_index = 1

            # å¤„ç†å‰©ä½™æ–°é—»æ¡ç›®
            for j in range(start_index, len(stat["titles"])):
                title_data = stat["titles"][j]
                if format_type == "wework":
                    formatted_title = format_title_for_platform(
                        "wework", title_data, show_source=True
                    )
                elif format_type == "telegram":
                    formatted_title = format_title_for_platform(
                        "telegram", title_data, show_source=True
                    )
                elif format_type == "ntfy":
                    formatted_title = format_title_for_platform(
                        "ntfy", title_data, show_source=True
                    )
                elif format_type == "feishu":
                    formatted_title = format_title_for_platform(
                        "feishu", title_data, show_source=True
                    )
                elif format_type == "dingtalk":
                    formatted_title = format_title_for_platform(
                        "dingtalk", title_data, show_source=True
                    )
                else:
                    formatted_title = f"{title_data['title']}"

                news_line = f"  {j + 1}. {formatted_title}\n"
                if j < len(stat["titles"]) - 1:
                    news_line += "\n"

                test_content = current_batch + news_line
                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    >= max_bytes
                ):
                    if current_batch_has_content:
                        batches.append(current_batch + base_footer)
                    current_batch = base_header + stats_header + word_header + news_line
                    current_batch_has_content = True
                else:
                    current_batch = test_content
                    current_batch_has_content = True

            # è¯ç»„é—´åˆ†éš”ç¬¦
            if i < len(report_data["stats"]) - 1:
                separator = ""
                if format_type == "wework":
                    separator = f"\n\n\n\n"
                elif format_type == "telegram":
                    separator = f"\n\n"
                elif format_type == "ntfy":
                    separator = f"\n\n"
                elif format_type == "feishu":
                    separator = f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\n"
                elif format_type == "dingtalk":
                    separator = f"\n---\n\n"

                test_content = current_batch + separator
                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    < max_bytes
                ):
                    current_batch = test_content

    # å¤„ç†æ–°å¢æ–°é—»ï¼ˆåŒæ ·ç¡®ä¿æ¥æºæ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»çš„åŸå­æ€§ï¼‰
    if report_data["new_titles"]:
        new_header = ""
        if format_type == "wework":
            new_header = f"\n\n\n\nğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
        elif format_type == "telegram":
            new_header = (
                f"\n\nğŸ†• æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—» (å…± {report_data['total_new_count']} æ¡)\n\n"
            )
        elif format_type == "ntfy":
            new_header = f"\n\nğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
        elif format_type == "feishu":
            new_header = f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\nğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
        elif format_type == "dingtalk":
            new_header = f"\n---\n\nğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"

        test_content = current_batch + new_header
        if (
            len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
            >= max_bytes
        ):
            if current_batch_has_content:
                batches.append(current_batch + base_footer)
            current_batch = base_header + new_header
            current_batch_has_content = True
        else:
            current_batch = test_content
            current_batch_has_content = True

        # é€ä¸ªå¤„ç†æ–°å¢æ–°é—»æ¥æº
        for source_data in report_data["new_titles"]:
            source_header = ""
            if format_type == "wework":
                source_header = f"**{source_data['source_name']}** ({len(source_data['titles'])} æ¡):\n\n"
            elif format_type == "telegram":
                source_header = f"{source_data['source_name']} ({len(source_data['titles'])} æ¡):\n\n"
            elif format_type == "ntfy":
                source_header = f"**{source_data['source_name']}** ({len(source_data['titles'])} æ¡):\n\n"
            elif format_type == "feishu":
                source_header = f"**{source_data['source_name']}** ({len(source_data['titles'])} æ¡):\n\n"
            elif format_type == "dingtalk":
                source_header = f"**{source_data['source_name']}** ({len(source_data['titles'])} æ¡):\n\n"

            # æ„å»ºç¬¬ä¸€æ¡æ–°å¢æ–°é—»
            first_news_line = ""
            if source_data["titles"]:
                first_title_data = source_data["titles"][0]
                title_data_copy = first_title_data.copy()
                title_data_copy["is_new"] = False

                if format_type == "wework":
                    formatted_title = format_title_for_platform(
                        "wework", title_data_copy, show_source=False
                    )
                elif format_type == "telegram":
                    formatted_title = format_title_for_platform(
                        "telegram", title_data_copy, show_source=False
                    )
                elif format_type == "feishu":
                    formatted_title = format_title_for_platform(
                        "feishu", title_data_copy, show_source=False
                    )
                elif format_type == "dingtalk":
                    formatted_title = format_title_for_platform(
                        "dingtalk", title_data_copy, show_source=False
                    )
                else:
                    formatted_title = f"{title_data_copy['title']}"

                first_news_line = f"  1. {formatted_title}\n"

            # åŸå­æ€§æ£€æŸ¥ï¼šæ¥æºæ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»
            source_with_first_news = source_header + first_news_line
            test_content = current_batch + source_with_first_news

            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                >= max_bytes
            ):
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + new_header + source_with_first_news
                current_batch_has_content = True
                start_index = 1
            else:
                current_batch = test_content
                current_batch_has_content = True
                start_index = 1

            # å¤„ç†å‰©ä½™æ–°å¢æ–°é—»
            for j in range(start_index, len(source_data["titles"])):
                title_data = source_data["titles"][j]
                title_data_copy = title_data.copy()
                title_data_copy["is_new"] = False

                if format_type == "wework":
                    formatted_title = format_title_for_platform(
                        "wework", title_data_copy, show_source=False
                    )
                elif format_type == "telegram":
                    formatted_title = format_title_for_platform(
                        "telegram", title_data_copy, show_source=False
                    )
                elif format_type == "feishu":
                    formatted_title = format_title_for_platform(
                        "feishu", title_data_copy, show_source=False
                    )
                elif format_type == "dingtalk":
                    formatted_title = format_title_for_platform(
                        "dingtalk", title_data_copy, show_source=False
                    )
                else:
                    formatted_title = f"{title_data_copy['title']}"

                news_line = f"  {j + 1}. {formatted_title}\n"

                test_content = current_batch + news_line
                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    >= max_bytes
                ):
                    if current_batch_has_content:
                        batches.append(current_batch + base_footer)
                    current_batch = base_header + new_header + source_header + news_line
                    current_batch_has_content = True
                else:
                    current_batch = test_content
                    current_batch_has_content = True

            current_batch += "\n"

    if report_data["failed_ids"]:
        failed_header = ""
        if format_type == "wework":
            failed_header = f"\n\n\n\nâš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
        elif format_type == "telegram":
            failed_header = f"\n\nâš ï¸ æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š\n\n"
        elif format_type == "ntfy":
            failed_header = f"\n\nâš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
        elif format_type == "feishu":
            failed_header = f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\nâš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
        elif format_type == "dingtalk":
            failed_header = f"\n---\n\nâš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"

        test_content = current_batch + failed_header
        if (
            len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
            >= max_bytes
        ):
            if current_batch_has_content:
                batches.append(current_batch + base_footer)
            current_batch = base_header + failed_header
            current_batch_has_content = True
        else:
            current_batch = test_content
            current_batch_has_content = True

        for i, id_value in enumerate(report_data["failed_ids"], 1):
            if format_type == "feishu":
                failed_line = f"  â€¢ <font color='red'>{id_value}</font>\n"
            elif format_type == "dingtalk":
                failed_line = f"  â€¢ **{id_value}**\n"
            else:
                failed_line = f"  â€¢ {id_value}\n"

            test_content = current_batch + failed_line
            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                >= max_bytes
            ):
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + failed_header + failed_line
                current_batch_has_content = True
            else:
                current_batch = test_content
                current_batch_has_content = True

    # å®Œæˆæœ€åæ‰¹æ¬¡
    if current_batch_has_content:
        batches.append(current_batch + base_footer)

    return batches


class NewsAnalyzer:
    """High-level workflow: crawl -> persist -> analyze -> notify."""

    def __init__(self):
        self.config = CONFIG
        self.report_mode = self.config["REPORT_MODE"]
        self.platforms = self._normalize_platforms()
        self.proxy_url = (
            self.config["DEFAULT_PROXY"] if self.config["USE_PROXY"] else None
        )
        self.fetcher = DataFetcher(self.proxy_url if self.config["USE_PROXY"] else None)

    def _normalize_platforms(self) -> List[Tuple[str, str]]:
        normalized: List[Tuple[str, str]] = []
        for platform in self.config.get("PLATFORMS", []):
            if isinstance(platform, str):
                platform_id = platform.strip()
                if platform_id:
                    normalized.append((platform_id, platform_id))
            elif isinstance(platform, dict):
                platform_id = (platform.get("id") or "").strip()
                if platform_id:
                    normalized.append((platform_id, platform.get("name", platform_id)))
        return normalized

    def _report_label(self) -> str:
        mapping = {
            "daily": "æ—¥å¸¸æ±‡æ€»",
            "current": "å½“å‰å¿«ç…§",
            "incremental": "å¢é‡æ’­æŠ¥",
        }
        return mapping.get(self.report_mode, "æ—¥å¸¸æ±‡æ€»")

    def run(self) -> None:
        if not self.platforms:
            print("æœªé…ç½®ä»»ä½•é‡‡é›†å¹³å°ï¼Œç»ˆæ­¢æ‰§è¡Œã€‚")
            return

        platform_ids = [pid for pid, _ in self.platforms]
        start_ts = time.time()

        fetched_results: Dict[str, Dict] = {}
        fetched_id_map: Dict[str, str] = {}
        failed_ids: List[str] = []

        if self.config["ENABLE_CRAWLER"]:
            print(f"å¼€å§‹æŠ“å– {len(self.platforms)} ä¸ªå¹³å°çš„æ•°æ®...")
            fetched_results, fetched_id_map, failed_ids = self.fetcher.crawl_websites(
                self.platforms, request_interval=self.config["REQUEST_INTERVAL"]
            )
            if fetched_results:
                txt_path = save_titles_to_file(
                    fetched_results, fetched_id_map, failed_ids
                )
                print(f"âœ… æœ€æ–°åŸå§‹æ•°æ®å·²å†™å…¥: {txt_path}")
            else:
                print("âš ï¸ æœªè·å–åˆ°æ–°çš„æ¦œå•æ•°æ®ï¼Œæœ¬æ¬¡å°†å°è¯•ä½¿ç”¨å†å²æ–‡ä»¶ã€‚")
        else:
            print("é…ç½®å·²å…³é—­é‡‡é›†åŠŸèƒ½ï¼Œå°†å°è¯•åŸºäºç°æœ‰æ•°æ®ç”ŸæˆæŠ¥å‘Šã€‚")

        aggregated_results, aggregated_id_map, title_info = read_all_today_titles(
            platform_ids
        )
        if not aggregated_results:
            if not fetched_results:
                print("æœªæ‰¾åˆ°å¯ç”¨äºåˆ†æçš„æ•°æ®ï¼Œæµç¨‹ä¸­æ­¢ã€‚")
                return
            aggregated_results = fetched_results
            aggregated_id_map = fetched_id_map
            title_info = {}

        new_titles = detect_latest_new_titles(platform_ids)
        word_groups, filter_words = load_frequency_words()
        stats, total_titles = count_word_frequency(
            aggregated_results,
            word_groups,
            filter_words,
            aggregated_id_map,
            title_info,
            rank_threshold=self.config["RANK_THRESHOLD"],
            new_titles=new_titles,
            mode=self.report_mode,
        )

        html_file = generate_html_report(
            stats,
            total_titles,
            failed_ids,
            new_titles,
            aggregated_id_map,
            self.report_mode,
            is_daily_summary=self.report_mode == "daily",
        )
        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

        if not self.config["ENABLE_NOTIFICATION"]:
            print("é€šçŸ¥åŠŸèƒ½å·²å…³é—­ï¼Œè·³è¿‡æ¨é€ã€‚")
            return

        send_to_notifications(
            stats,
            failed_ids,
            self._report_label(),
            new_titles,
            aggregated_id_map,
            self.proxy_url,
            self.report_mode,
            html_file,
        )
        print(f"ğŸ¯ å·²å®Œæˆæ•´æ¡é“¾è·¯ï¼Œè€—æ—¶ {time.time() - start_ts:.2f} ç§’ã€‚")


def send_to_notifications(
    stats: List[Dict],
    failed_ids: Optional[List] = None,
    report_type: str = "æ—¥å¸¸æ±‡æ€»",
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
    html_file_path: Optional[str] = None,
) -> Dict[str, bool]:
    """Send notifications to WeCom only"""
    results: Dict[str, bool] = {}

    if CONFIG["PUSH_WINDOW"]["ENABLED"]:
        push_manager = PushRecordManager()
        time_range_start = CONFIG["PUSH_WINDOW"]["TIME_RANGE"]["START"]
        time_range_end = CONFIG["PUSH_WINDOW"]["TIME_RANGE"]["END"]

        if not push_manager.is_in_time_range(time_range_start, time_range_end):
            now = get_beijing_time()
            print(
                f"æ¨é€å¤„äºé™åˆ¶æ—¶é—´æ®µï¼Œå½“å‰æ—¶é—´ {now.strftime('%H:%M')} ä¸åœ¨ {time_range_start}-{time_range_end} å†…ï¼Œè·³è¿‡å‘é€"
            )
            return results

        if CONFIG["PUSH_WINDOW"]["ONCE_PER_DAY"] and push_manager.has_pushed_today():
            print("æ¨é€å¤„äºé™åˆ¶æ¨¡å¼ï¼Œä»Šå¤©å·²æ¨é€è¿‡ï¼Œè·³è¿‡å‘é€")
            return results

    report_data = prepare_report_data(stats, failed_ids, new_titles, id_to_name, mode)
    wework_url = CONFIG["WEWORK_WEBHOOK_URL"]

    if not wework_url:
        print("æœªé…ç½®ä¼ä¸šå¾®ä¿¡ Webhookï¼Œè·³è¿‡é€šçŸ¥é˜¶æ®µ")
        return results

    results["wework"] = send_to_wework(
        wework_url, report_data, report_type, proxy_url, mode
    )

    if (
        CONFIG["PUSH_WINDOW"]["ENABLED"]
        and CONFIG["PUSH_WINDOW"]["ONCE_PER_DAY"]
        and any(results.values())
    ):
        push_manager = PushRecordManager()
        push_manager.record_push(report_type)

    return results



def send_to_wework(
    webhook_url: str,
    report_data: Dict,
    report_type: str,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
) -> bool:
    """å‘é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆæ”¯æŒåˆ†æ‰¹å‘é€ï¼‰"""
    headers = {"Content-Type": "application/json"}
    proxies = None
    if proxy_url:
        proxies = {"http": proxy_url, "https": proxy_url}

    # è·å–åˆ†æ‰¹å†…å®¹

    batches = split_content_into_batches(report_data, "wework", mode=mode)

    print(f"ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯åˆ†ä¸º {len(batches)} æ‰¹æ¬¡å‘é€ [{report_type}]")

    # é€æ‰¹å‘é€
    for i, batch_content in enumerate(batches, 1):
        batch_size = len(batch_content.encode("utf-8"))
        print(
            f"å‘é€ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡ï¼Œå¤§å°ï¼š{batch_size} å­—èŠ‚ [{report_type}]"
        )

        # æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
        if len(batches) > 1:
            batch_header = f"**[ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡]**\n\n"
            batch_content = batch_header + batch_content

        payload = {"msgtype": "markdown", "markdown": {"content": batch_content}}

        try:
            response = requests.post(
                webhook_url, headers=headers, json=payload, proxies=proxies, timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                if result.get("errcode") == 0:
                    print(f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€æˆåŠŸ [{report_type}]")
                    # æ‰¹æ¬¡é—´é—´éš”
                    if i < len(batches):
                        time.sleep(CONFIG["BATCH_SEND_INTERVAL"])
                else:
                    print(
                        f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{result.get('errmsg')}"
                    )
                    return False
            else:
                print(
                    f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                )
                return False
        except Exception as e:
            print(f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
            return False

    print(f"ä¼ä¸šå¾®ä¿¡æ‰€æœ‰ {len(batches)} æ‰¹æ¬¡å‘é€å®Œæˆ [{report_type}]")
    return True


def main():
    try:
        analyzer = NewsAnalyzer()
        analyzer.run()
    except FileNotFoundError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        print("  â€¢ config/config.yaml")
        print("  â€¢ config/frequency_words.txt")
        print("\nå‚è€ƒé¡¹ç›®æ–‡æ¡£è¿›è¡Œæ­£ç¡®é…ç½®")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
